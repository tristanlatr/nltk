<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  

  <head>
    
    <title>nltk.tokenize</title>
    <meta name="generator" content="pydoctor 21.2.2"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    

    <nav class="navbar navbar-default">
  
  <div class="container">

    <div class="navbar-header">
      
      <div class="navlinks">
          <span class="navbar-brand">
            <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a> <a href="index.html">API Documentation</a>
          </span>

          <a href="moduleIndex.html">
            Modules
          </a>

          <a href="classIndex.html">
            Classes
          </a>

          <a href="nameIndex.html">
            Names
          </a>
      </div>

    </div>
  </div>
</nav>

    

    <div class="container">

      <div class="page-header">
        <h1 class="package"><code><code><a href="nltk.html">nltk</a></code><wbr></wbr>.<code><a href="nltk.tokenize.html">tokenize</a></code></code></h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <div class="categoryHeader">
        package documentation
      </div>

      <div class="extrasDocstring">
        <a href="https://github.com/tristanlatr/nltk/tree/f83d0acf07ed0fd627306ec5ae73265b05025e51/nltk/tokenize/__init__.py" class="sourceLink">(source)</a>
        <p></p>
      </div>

      <div class="moduleDocstring">
        <div><p class="pre">NLTK Tokenizer Package

Tokenizers divide strings into lists of substrings.  For example,
tokenizers can be used to find the words and punctuation in a string:

    &gt;&gt;&gt; from nltk.tokenize import word_tokenize
    &gt;&gt;&gt; s = '''Good muffins cost $3.88\nin New York.  Please buy me
    ... two of them.\n\nThanks.'''
    &gt;&gt;&gt; word_tokenize(s)
    ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.',
    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']

This particular tokenizer requires the Punkt sentence tokenization
models to be installed. NLTK also provides a simpler,
regular-expression based tokenizer, which splits text on whitespace
and punctuation:

    &gt;&gt;&gt; from nltk.tokenize import wordpunct_tokenize
    &gt;&gt;&gt; wordpunct_tokenize(s)
    ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.',
    'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']

We can also operate at the level of sentences, using the sentence
tokenizer directly as follows:

    &gt;&gt;&gt; from nltk.tokenize import sent_tokenize, word_tokenize
    &gt;&gt;&gt; sent_tokenize(s)
    ['Good muffins cost $3.88\nin New York.', 'Please buy me\ntwo of them.', 'Thanks.']
    &gt;&gt;&gt; [word_tokenize(t) for t in sent_tokenize(s)]
    [['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],
    ['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]

Caution: when tokenizing a Unicode string, make sure you are not
using an encoded version of the string (it may be necessary to
decode it first, e.g. with ``s.decode("utf8")``.

NLTK tokenizers can produce token-spans, represented as tuples of integers
having the same semantics as string slices, to support efficient comparison
of tokenizers.  (These methods are implemented as generators.)

    &gt;&gt;&gt; from nltk.tokenize import WhitespaceTokenizer
    &gt;&gt;&gt; list(WhitespaceTokenizer().span_tokenize(s))
    [(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36), (38, 44),
    (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]

There are numerous ways to tokenize text.  If you need more control over
tokenization, see the other methods provided in this package.

For further information, please see Chapter 3 of the NLTK book.</p></div>
      </div>

      <div id="splitTables">
        <table class="children sortable" id="id1775">
  
  
  <tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.api.html">api</a></code></td>
    <td><span>Tokenizer Interface</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.casual.html">casual</a></code></td>
    <td><span>Twitter-aware tokenizer, designed to be flexible and easy to adapt to new domains and tasks. The basic logic is this:</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.destructive.html">destructive</a></code></td>
    <td><span class="undocumented">No module docstring; 2/2 classes documented</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.mwe.html">mwe</a></code></td>
    <td><span>Multi-Word Expression Tokenizer</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.nist.html">nist</a></code></td>
    <td><span class="undocumented">No summary</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.punkt.html">punkt</a></code></td>
    <td><span>Punkt Sentence Tokenizer</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.regexp.html">regexp</a></code></td>
    <td><span>Regular-Expression Tokenizers</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.repp.html">repp</a></code></td>
    <td><span class="undocumented">No module docstring; 1/1 class documented</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.sexpr.html">sexpr</a></code></td>
    <td><span>S-Expression Tokenizer</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.simple.html">simple</a></code></td>
    <td><span>Simple Tokenizers</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.sonority_sequencing.html">sonority_sequencing</a></code></td>
    <td><span class="undocumented">No summary</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.stanford.html">stanford</a></code></td>
    <td><span class="undocumented">No module docstring; 0/1 variable, 0/1 function, 1/1 class documented</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.stanford_segmenter.html">stanford_segmenter</a></code></td>
    <td><span class="undocumented">No module docstring; 0/1 variable, 0/1 function, 1/1 class documented</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.texttiling.html">texttiling</a></code></td>
    <td><span class="undocumented">No module docstring; 0/5 variable, 1/2 function, 3/3 classes documented</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.toktok.html">toktok</a></code></td>
    <td><span>The tok-tok tokenizer is a simple, general tokenizer, where the input has one sentence per line; thus only final period is tokenized.</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.treebank.html">treebank</a></code></td>
    <td><span>Penn Treebank Tokenizer</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.tokenize.util.html">util</a></code></td>
    <td><span class="undocumented">No module docstring; 7/7 functions, 1/1 class documented</span></td>
  </tr>
</table>
        

          <p class="fromInitPy">From <code>__init__.py</code>:</p><table class="children sortable" id="id1776">
  
  
  <tr class="function">
    
    <td>Function</td>
    <td><code><a href="#sent_tokenize">sent_tokenize</a></code></td>
    <td><span class="undocumented">No summary</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#word_tokenize">word_tokenize</a></code></td>
    <td><span class="undocumented">No summary</span></td>
  </tr><tr class="variable private">
    
    <td>Variable</td>
    <td><code><a href="#_treebank_word_tokenizer">_treebank_word_tokenizer</a></code></td>
    <td><span class="undocumented">Undocumented</span></td>
  </tr>
</table>
      </div>

      <div id="childList">

        <div class="basefunction">
  
  
  <a name="nltk.tokenize.sent_tokenize">
    
  </a>
  <a name="sent_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">sent_tokenize</span>(text, language='english'):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/f83d0acf07ed0fd627306ec5ae73265b05025e51/nltk/tokenize/__init__.py#L96">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p class="pre">Return a sentence-tokenized copy of *text*,
using NLTK's recommended sentence tokenizer
(currently :class:`.PunktSentenceTokenizer`
for the specified language).

:param text: text to split into sentences
:param language: the model name in the Punkt corpus</p></div>
  </div>
</div><div class="basevariable private">
  
  
  <a name="nltk.tokenize._treebank_word_tokenizer">
    
  </a>
  <a name="_treebank_word_tokenizer">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-defname">_treebank_word_tokenizer</span> =
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/f83d0acf07ed0fd627306ec5ae73265b05025e51/nltk/tokenize/__init__.py#L111">
      
      (source)
    </a>
  </div>
  <div class="functionBody">
    
    
    <div><p class="undocumented">Undocumented</p></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.word_tokenize">
    
  </a>
  <a name="word_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">word_tokenize</span>(text, language='english', preserve_line=False):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/f83d0acf07ed0fd627306ec5ae73265b05025e51/nltk/tokenize/__init__.py#L114">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p class="pre">Return a tokenized copy of *text*,
using NLTK's recommended word tokenizer
(currently an improved :class:`.TreebankWordTokenizer`
along with :class:`.PunktSentenceTokenizer`
for the specified language).

:param text: text to split into words
:type text: str
:param language: the model name in the Punkt corpus
:type language: str
:param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.
:type preserve_line: bool</p></div>
  </div>
</div>

      </div>
    </div>

    <footer class="navbar navbar-default">
  
  <div class="container">
    <a href="index.html">API Documentation</a> for <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a>,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    21.2.2 at 2021-06-22 02:51:08.
  </div>
</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>