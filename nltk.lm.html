<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  

  <head>
    
    <title>nltk.lm</title>
    <meta name="generator" content="pydoctor 21.2.2"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    

    <nav class="navbar navbar-default">
  
  <div class="container">

    <div class="navbar-header">
      
      <div class="navlinks">
          <span class="navbar-brand">
            <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a> <a href="index.html">API Documentation</a>
          </span>

          <a href="moduleIndex.html">
            Modules
          </a>

          <a href="classIndex.html">
            Classes
          </a>

          <a href="nameIndex.html">
            Names
          </a>
      </div>

    </div>
  </div>
</nav>

    

    <div class="container">

      <div class="page-header">
        <h1 class="package"><code><code><a href="nltk.html">nltk</a></code><wbr></wbr>.<code><a href="nltk.lm.html">lm</a></code></code></h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <div class="categoryHeader">
        package documentation
      </div>

      <div class="extrasDocstring">
        <a href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/lm/__init__.py" class="sourceLink">(source)</a>
        <p></p>
      </div>

      <div class="moduleDocstring">
        <div><p>Currently this module covers only ngram language models, but it should be easy
to extend to neural models.</p>
<div class="rst-section" id="rst-preparing-data">
<h1 class="heading">Preparing Data</h1>
<p>Before we train our ngram models it is necessary to make sure the data we put in
them is in the right format.
Let's say we have a text that is a list of sentences, where each sentence is
a list of strings. For simplicity we just consider a text consisting of
characters instead of words.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>text = [[<span class="py-string">'a'</span>, <span class="py-string">'b'</span>, <span class="py-string">'c'</span>], [<span class="py-string">'a'</span>, <span class="py-string">'c'</span>, <span class="py-string">'d'</span>, <span class="py-string">'c'</span>, <span class="py-string">'e'</span>, <span class="py-string">'f'</span>]]</pre></blockquote>
<p>If we want to train a bigram model, we need to turn this text into bigrams.
Here's what the first sentence of our text would look like if we use a function
from NLTK for this.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.util <span class="py-keyword">import</span> bigrams
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(bigrams(text[0]))
<span class="py-output">[('a', 'b'), ('b', 'c')]</span>
</pre></blockquote>
<p>Notice how "b" occurs both as the first and second member of different bigrams
but "a" and "c" don't? Wouldn't it be nice to somehow indicate how often sentences
start with "a" and end with "c"?
A standard way to deal with this is to add special "padding" symbols to the
sentence before splitting it into ngrams.
Fortunately, NLTK also has a function for that, let's see what it does to the
first sentence.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.util <span class="py-keyword">import</span> pad_sequence
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(pad_sequence(text[0],
<span class="py-more">... </span>pad_left=<span class="py-builtin">True</span>,
<span class="py-more">... </span>left_pad_symbol=<span class="py-string">"&lt;s&gt;"</span>,
<span class="py-more">... </span>pad_right=<span class="py-builtin">True</span>,
<span class="py-more">... </span>right_pad_symbol=<span class="py-string">"&lt;/s&gt;"</span>,
<span class="py-more">... </span>n=2))
<span class="py-output">['&lt;s&gt;', 'a', 'b', 'c', '&lt;/s&gt;']</span>
</pre></blockquote>
<p>Note the <code>n</code> argument, that tells the function we need padding for bigrams.
Now, passing all these parameters every time is tedious and in most cases they
can be safely assumed as defaults anyway.
Thus our module provides a convenience function that has all these arguments
already set while the other arguments remain the same as for <code><a href="nltk.util.html#pad_sequence">pad_sequence</a></code>.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.lm.preprocessing <span class="py-keyword">import</span> pad_both_ends
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(pad_both_ends(text[0], n=2))
<span class="py-output">['&lt;s&gt;', 'a', 'b', 'c', '&lt;/s&gt;']</span>
</pre></blockquote>
<p>Combining the two parts discussed so far we get the following preparation steps
for one sentence.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(bigrams(pad_both_ends(text[0], n=2)))
<span class="py-output">[('&lt;s&gt;', 'a'), ('a', 'b'), ('b', 'c'), ('c', '&lt;/s&gt;')]</span>
</pre></blockquote>
<p>To make our model more robust we could also train it on unigrams (single words)
as well as bigrams, its main source of information.
NLTK once again helpfully provides a function called <code><a href="nltk.util.html#everygrams">everygrams</a></code>.
While not the most efficient, it is conceptually simple.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.util <span class="py-keyword">import</span> everygrams
<span class="py-prompt">&gt;&gt;&gt; </span>padded_bigrams = <span class="py-builtin">list</span>(pad_both_ends(text[0], n=2))
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(everygrams(padded_bigrams, max_len=2))
<span class="py-output">[('&lt;s&gt;',), ('&lt;s&gt;', 'a'), ('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',), ('c', '&lt;/s&gt;'), ('&lt;/s&gt;',)]</span>
</pre></blockquote>
<p>We are almost ready to start counting ngrams, just one more step left.
During training and evaluation our model will rely on a vocabulary that
defines which words are "known" to the model.
To create this vocabulary we need to pad our sentences (just like for counting
ngrams) and then combine the sentences into one flat stream of words.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.lm.preprocessing <span class="py-keyword">import</span> flatten
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(flatten(pad_both_ends(sent, n=2) <span class="py-keyword">for</span> sent <span class="py-keyword">in</span> text))
<span class="py-output">['&lt;s&gt;', 'a', 'b', 'c', '&lt;/s&gt;', '&lt;s&gt;', 'a', 'c', 'd', 'c', 'e', 'f', '&lt;/s&gt;']</span>
</pre></blockquote>
<p>In most cases we want to use the same text as the source for both vocabulary
and ngram counts.
Now that we understand what this means for our preprocessing, we can simply import
a function that does everything for us.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.lm.preprocessing <span class="py-keyword">import</span> padded_everygram_pipeline
<span class="py-prompt">&gt;&gt;&gt; </span>train, vocab = padded_everygram_pipeline(2, text)</pre></blockquote>
<p>So as to avoid re-creating the text in memory, both <code>train</code> and <code>vocab</code> are lazy
iterators. They are evaluated on demand at training time.</p>
</div>
<div class="rst-section" id="rst-training">
<h1 class="heading">Training</h1>
<p>Having prepared our data we are ready to start training a model.
As a simple example, let us train a Maximum Likelihood Estimator (MLE).
We only need to specify the highest ngram order to instantiate it.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.lm <span class="py-keyword">import</span> MLE
<span class="py-prompt">&gt;&gt;&gt; </span>lm = MLE(2)</pre></blockquote>
<p>This automatically creates an empty vocabulary...</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">len</span>(lm.vocab)
<span class="py-output">0</span>
</pre></blockquote>
<p>... which gets filled as we fit the model.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.fit(train, vocab)
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">print</span>(lm.vocab)
<span class="py-output">&lt;Vocabulary with cutoff=1 unk_label='&lt;UNK&gt;' and 9 items&gt;</span>
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">len</span>(lm.vocab)
<span class="py-output">9</span>
</pre></blockquote>
<p>The vocabulary helps us handle words that have not occurred during training.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.vocab.lookup(text[0])
<span class="py-output">('a', 'b', 'c')</span>
<span class="py-prompt">&gt;&gt;&gt; </span>lm.vocab.lookup([<span class="py-string">"aliens"</span>, <span class="py-string">"from"</span>, <span class="py-string">"Mars"</span>])
<span class="py-output">('&lt;UNK&gt;', '&lt;UNK&gt;', '&lt;UNK&gt;')</span>
</pre></blockquote>
<p>Moreover, in some cases we want to ignore words that we did see during training
but that didn't occur frequently enough, to provide us useful information.
You can tell the vocabulary to ignore such words.
To find out how that works, check out the docs for the <code><a href="nltk.lm.Vocabulary.html">Vocabulary</a></code> class.</p>
</div>
<div class="rst-section" id="rst-using-a-trained-model">
<h1 class="heading">Using a Trained Model</h1>
<p>When it comes to ngram models the training boils down to counting up the ngrams
from the training corpus.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">print</span>(lm.counts)
<span class="py-output">&lt;NgramCounter with 2 ngram orders and 24 ngrams&gt;</span>
</pre></blockquote>
<p>This provides a convenient interface to access counts for unigrams...</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.counts[<span class="py-string">'a'</span>]
<span class="py-output">2</span>
</pre></blockquote>
<p>...and bigrams (in this case "a b")</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.counts[[<span class="py-string">'a'</span>]][<span class="py-string">'b'</span>]
<span class="py-output">1</span>
</pre></blockquote>
<p>And so on. However, the real purpose of training a language model is to have it
score how probable words are in certain contexts.
This being MLE, the model returns the item's relative frequency as its score.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.score(<span class="py-string">"a"</span>)
<span class="py-output">0.15384615384615385</span>
</pre></blockquote>
<p>Items that are not seen during training are mapped to the vocabulary's
"unknown label" token. This is "&lt;UNK&gt;" by default.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.score(<span class="py-string">"&lt;UNK&gt;"</span>) == lm.score(<span class="py-string">"aliens"</span>)
<span class="py-output">True</span>
</pre></blockquote>
<p>Here's how you get the score for a word given some preceding context.
For example we want to know what is the chance that "b" is preceded by "a".</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.score(<span class="py-string">"b"</span>, [<span class="py-string">"a"</span>])
<span class="py-output">0.5</span>
</pre></blockquote>
<p>To avoid underflow when working with many small score values it makes sense to
take their logarithm.
For convenience this can be done with the <code>logscore</code> method.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.logscore(<span class="py-string">"a"</span>)
<span class="py-output">-2.700439718141092</span>
</pre></blockquote>
<p>Building on this method, we can also evaluate our model's cross-entropy and
perplexity with respect to sequences of ngrams.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>test = [(<span class="py-string">'a'</span>, <span class="py-string">'b'</span>), (<span class="py-string">'c'</span>, <span class="py-string">'d'</span>)]
<span class="py-prompt">&gt;&gt;&gt; </span>lm.entropy(test)
<span class="py-output">1.292481250360578</span>
<span class="py-prompt">&gt;&gt;&gt; </span>lm.perplexity(test)
<span class="py-output">2.449489742783178</span>
</pre></blockquote>
<p>It is advisable to preprocess your test text exactly the same way as you did
the training text.</p>
<p>One cool feature of ngram models is that they can be used to generate text.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.generate(1, random_seed=3)
<span class="py-output">'&lt;s&gt;'</span>
<span class="py-prompt">&gt;&gt;&gt; </span>lm.generate(5, random_seed=3)
<span class="py-output">['&lt;s&gt;', 'a', 'b', 'c', 'd']</span>
</pre></blockquote>
<p>Provide <code>random_seed</code> if you want to consistently reproduce the same text all
other things being equal. Here we are using it to test the examples.</p>
<p>You can also condition your generation on some preceding text with the <code>context</code>
argument.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>lm.generate(5, text_seed=[<span class="py-string">'c'</span>], random_seed=3)
<span class="py-output">['&lt;/s&gt;', 'c', 'd', 'c', 'd']</span>
</pre></blockquote>
<p>Note that an ngram model is restricted in how much preceding context it can
take into account. For example, a trigram model can only condition its output
on 2 preceding words. If you pass in a 4-word context, the first two words
will be ignored.</p>
</div>
</div>
      </div>

      <div id="splitTables">
        <table class="children sortable" id="id778">
  
  
  <tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.lm.api.html">api</a></code></td>
    <td><span>Language Model Interface.</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.lm.counter.html">counter</a></code></td>
    <td><span>Language Model Counter ----------------------</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.lm.models.html">models</a></code></td>
    <td><span>Language Models</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.lm.preprocessing.html">preprocessing</a></code></td>
    <td><span class="undocumented">No module docstring; 1/1 variable, 2/2 functions documented</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.lm.smoothing.html">smoothing</a></code></td>
    <td><span>Smoothing algorithms for language modeling.</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.lm.util.html">util</a></code></td>
    <td><span>Language Model Utilities</span></td>
  </tr><tr class="module">
    
    <td>Module</td>
    <td><code><a href="nltk.lm.vocabulary.html">vocabulary</a></code></td>
    <td><span>Language Model Vocabulary</span></td>
  </tr>
</table>
        

          <p class="fromInitPy">From <code>__init__.py</code>:</p><table class="children sortable" id="id779">
  
  
  <tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.lm.KneserNeyInterpolated.html">KneserNeyInterpolated</a></code></td>
    <td><span>Interpolated version of Kneser-Ney smoothing.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.lm.Laplace.html">Laplace</a></code></td>
    <td><span>Implements Laplace (add one) smoothing.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.lm.Lidstone.html">Lidstone</a></code></td>
    <td><span>Provides Lidstone-smoothed scores.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.lm.MLE.html">MLE</a></code></td>
    <td><span>Class for providing MLE ngram model scores.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.lm.NgramCounter.html">NgramCounter</a></code></td>
    <td><span>Class for counting ngrams.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.lm.Vocabulary.html">Vocabulary</a></code></td>
    <td><span>Stores language model vocabulary.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.lm.WittenBellInterpolated.html">WittenBellInterpolated</a></code></td>
    <td><span>Interpolated version of Witten-Bell smoothing.</span></td>
  </tr>
</table>
      </div>

      <div id="childList">

        

      </div>
    </div>

    <footer class="navbar navbar-default">
  
  <div class="container">
    <a href="index.html">API Documentation</a> for <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a>,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    21.2.2 at 2021-06-22 02:56:13.
  </div>
</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>