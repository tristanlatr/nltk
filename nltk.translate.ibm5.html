<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  

  <head>
    
    <title>nltk.translate.ibm5</title>
    <meta name="generator" content="pydoctor 21.2.2"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    

    <nav class="navbar navbar-default">
  
  <div class="container">

    <div class="navbar-header">
      
      <div class="navlinks">
          <span class="navbar-brand">
            <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a> <a href="index.html">API Documentation</a>
          </span>

          <a href="moduleIndex.html">
            Modules
          </a>

          <a href="classIndex.html">
            Classes
          </a>

          <a href="nameIndex.html">
            Names
          </a>
      </div>

    </div>
  </div>
</nav>

    

    <div class="container">

      <div class="page-header">
        <h1 class="module"><code><code><a href="nltk.html">nltk</a></code><wbr></wbr>.<code><a href="nltk.translate.html">translate</a></code><wbr></wbr>.<code><a href="nltk.translate.ibm5.html">ibm5</a></code></code></h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <div class="categoryHeader">
        module documentation
      </div>

      <div class="extrasDocstring">
        <a href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/translate/ibm5.py" class="sourceLink">(source)</a>
        <p></p>
      </div>

      <div class="moduleDocstring">
        <div><p>Translation model that keeps track of vacant positions in the target
sentence to decide where to place translated words.</p>
<p>Translation can be viewed as a process where each word in the source
sentence is stepped through sequentially, generating translated words
for each source word. The target sentence can be viewed as being made
up of <tt class="rst-docutils literal">m</tt> empty slots initially, which gradually fill up as generated
words are placed in them.</p>
<p>Models 3 and 4 use distortion probabilities to decide how to place
translated words. For simplicity, these models ignore the history of
which slots have already been occupied with translated words.
Consider the placement of the last translated word: there is only one
empty slot left in the target sentence, so the distortion probability
should be 1.0 for that position and 0.0 everywhere else. However, the
distortion probabilities for Models 3 and 4 are set up such that all
positions are under consideration.</p>
<p>IBM Model 5 fixes this deficiency by accounting for occupied slots
during translation. It introduces the vacancy function v(j), the number
of vacancies up to, and including, position j in the target sentence.</p>
<p>Terminology:
Maximum vacancy:</p>
<blockquote>
The number of valid slots that a word can be placed in.
This is not necessarily the same as the number of vacant slots.
For example, if a tablet contains more than one word, the head word
cannot be placed at the last vacant slot because there will be no
space for the other words in the tablet. The number of valid slots
has to take into account the length of the tablet.
Non-head words cannot be placed before the head word, so vacancies
to the left of the head word are ignored.</blockquote>
<dl class="rst-docutils">
<dt>Vacancy difference:</dt>
<dd>For a head word: (v(j) - v(center of previous cept))
Can be positive or negative.
For a non-head word: (v(j) - v(position of previously placed word))
Always positive, because successive words in a tablet are assumed to
appear to the right of the previous word.</dd>
</dl>
<p>Positioning of target words fall under three cases:
(1) Words generated by NULL are distributed uniformly
(2) For a head word t, its position is modeled by the probability</p>
<blockquote>
v_head(dv | max_v,word_class_t(t))</blockquote>
<ol class="rst-arabic simple" start="3">
<li>For a non-head word t, its position is modeled by the probability
v_non_head(dv | max_v,word_class_t(t))</li>
</ol>
<p>dv and max_v are defined differently for head and non-head words.</p>
<p>The EM algorithm used in Model 5 is:
E step - In the training data, collect counts, weighted by prior</p>
<blockquote>
<p>probabilities.
(a) count how many times a source language word is translated</p>
<blockquote>
into a target language word</blockquote>
<ol class="rst-loweralpha simple" start="2">
<li>for a particular word class and maximum vacancy, count how
many times a head word and the previous cept's center have
a particular difference in number of vacancies</li>
</ol>
<ol class="rst-loweralpha simple" start="2">
<li>for a particular word class and maximum vacancy, count how
many times a non-head word and the previous target word
have a particular difference in number of vacancies</li>
</ol>
<ol class="rst-loweralpha simple" start="4">
<li>count how many times a source word is aligned to phi number
of target words</li>
<li>count how many times NULL is aligned to a target word</li>
</ol>
</blockquote>
<p>M step - Estimate new probabilities based on the counts from the E step</p>
<p>Like Model 4, there are too many possible alignments to consider. Thus,
a hill climbing approach is used to sample good candidates. In addition,
pruning is used to weed out unlikely alignments based on Model 4 scores.</p>
<p>Notations:
i: Position in the source sentence</p>
<blockquote>
Valid values are 0 (for NULL), 1, 2, ..., length of source sentence</blockquote>
<dl class="rst-docutils">
<dt>j: Position in the target sentence</dt>
<dd>Valid values are 1, 2, ..., length of target sentence</dd>
</dl>
<p>l: Number of words in the source sentence, excluding NULL
m: Number of words in the target sentence
s: A word in the source language
t: A word in the target language
phi: Fertility, the number of target words produced by a source word
p1: Probability that a target word produced by a source word is</p>
<blockquote>
accompanied by another target word that is aligned to NULL</blockquote>
<p>p0: 1 - p1
max_v: Maximum vacancy
dv: Vacancy difference, Î”v</p>
<p>The definition of v_head here differs from GIZA++, section 4.7 of
[Brown et al., 1993], and [Koehn, 2010]. In the latter cases, v_head is
v_head(v(j) | v(center of previous cept),max_v,word_class(t)).</p>
<p>Here, we follow appendix B of [Brown et al., 1993] and combine v(j) with
v(center of previous cept) to obtain dv:
v_head(v(j) - v(center of previous cept) | max_v,word_class(t)).</p>
<p>References:
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York.</p>
<p>Peter E Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and
Robert L. Mercer. 1993. The Mathematics of Statistical Machine
Translation: Parameter Estimation. Computational Linguistics, 19 (2),
263-311.</p>
</div>
      </div>

      <div id="splitTables">
        <table class="children sortable" id="id1893">
  
  
  <tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.translate.ibm5.IBMModel5.html">IBMModel5</a></code></td>
    <td><span>Translation model that keeps track of vacant positions in the target sentence to decide where to place translated words</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.translate.ibm5.Model5Counts.html">Model5Counts</a></code></td>
    <td><span>Data object to store counts of various parameters during training. Includes counts for vacancies.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.translate.ibm5.Slots.html">Slots</a></code></td>
    <td><span>Represents positions in a target sentence. Used to keep track of which slot (position) is occupied.</span></td>
  </tr>
</table>
        

          
      </div>

      <div id="childList">

        

      </div>
    </div>

    <footer class="navbar navbar-default">
  
  <div class="container">
    <a href="index.html">API Documentation</a> for <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a>,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    21.2.2 at 2021-06-22 02:56:13.
  </div>
</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>