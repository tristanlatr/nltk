<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  

  <head>
    
    <title>nltk.tokenize.regexp</title>
    <meta name="generator" content="pydoctor 21.2.2"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    

    <nav class="navbar navbar-default">
  
  <div class="container">

    <div class="navbar-header">
      
      <div class="navlinks">
          <span class="navbar-brand">
            <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a> <a href="index.html">API Documentation</a>
          </span>

          <a href="moduleIndex.html">
            Modules
          </a>

          <a href="classIndex.html">
            Classes
          </a>

          <a href="nameIndex.html">
            Names
          </a>
      </div>

    </div>
  </div>
</nav>

    

    <div class="container">

      <div class="page-header">
        <h1 class="module"><code><code><a href="nltk.html">nltk</a></code><wbr></wbr>.<code><a href="nltk.tokenize.html">tokenize</a></code><wbr></wbr>.<code><a href="nltk.tokenize.regexp.html">regexp</a></code></code></h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <div class="categoryHeader">
        module documentation
      </div>

      <div class="extrasDocstring">
        <a href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/regexp.py" class="sourceLink">(source)</a>
        <p></p>
      </div>

      <div class="moduleDocstring">
        <div><p>Regular-Expression Tokenizers</p>
<p>A <tt class="rst-docutils literal">RegexpTokenizer</tt> splits a string into substrings using a regular expression.
For example, the following tokenizer forms tokens out of alphabetic sequences,
money expressions, and any other non-whitespace sequences:</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize <span class="py-keyword">import</span> RegexpTokenizer
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">"Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.\n\nThanks."</span>
<span class="py-prompt">&gt;&gt;&gt; </span>tokenizer = RegexpTokenizer(<span class="py-string">'\w+|\$[\d\.]+|\S+'</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>tokenizer.tokenize(s)
<span class="py-output">['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',</span>
<span class="py-output">'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']</span>
</pre></blockquote>
<p>A <tt class="rst-docutils literal">RegexpTokenizer</tt> can use its regexp to match delimiters instead:</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>tokenizer = RegexpTokenizer(<span class="py-string">'\s+'</span>, gaps=<span class="py-builtin">True</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>tokenizer.tokenize(s)
<span class="py-output">['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.',</span>
<span class="py-output">'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']</span>
</pre></blockquote>
<p>Note that empty tokens are not returned when the delimiter appears at
the start or end of the string.</p>
<p>The material between the tokens is discarded.  For example,
the following tokenizer selects just the capitalized words:</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>capword_tokenizer = RegexpTokenizer(<span class="py-string">'[A-Z]\w+'</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>capword_tokenizer.tokenize(s)
<span class="py-output">['Good', 'New', 'York', 'Please', 'Thanks']</span>
</pre></blockquote>
<p>This module contains several subclasses of <tt class="rst-docutils literal">RegexpTokenizer</tt>
that use pre-defined regular expressions.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize <span class="py-keyword">import</span> BlanklineTokenizer
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-comment"># Uses '\s*\n\s*\n\s*':</span>
<span class="py-prompt">&gt;&gt;&gt; </span>BlanklineTokenizer().tokenize(s)
<span class="py-output">['Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.',</span>
<span class="py-output">'Thanks.']</span>
</pre></blockquote>
<p>All of the regular expression tokenizers are also available as functions:</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize <span class="py-keyword">import</span> regexp_tokenize, wordpunct_tokenize, blankline_tokenize
<span class="py-prompt">&gt;&gt;&gt; </span>regexp_tokenize(s, pattern=<span class="py-string">'\w+|\$[\d\.]+|\S+'</span>)
<span class="py-output">['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.',</span>
<span class="py-output">'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']</span>
<span class="py-prompt">&gt;&gt;&gt; </span>wordpunct_tokenize(s)
<span class="py-output">['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',</span>
<span class="py-output"> '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']</span>
<span class="py-prompt">&gt;&gt;&gt; </span>blankline_tokenize(s)
<span class="py-output">['Good muffins cost $3.88\nin New York.  Please buy me\ntwo of them.', 'Thanks.']</span>
</pre></blockquote>
<p>Caution: The function <tt class="rst-docutils literal">regexp_tokenize()</tt> takes the text as its
first argument, and the regular expression pattern as its second
argument.  This differs from the conventions used by Python's
<tt class="rst-docutils literal">re</tt> functions, where the pattern is always the first argument.
(This is for consistency with the other NLTK tokenizers.)</p>
</div>
      </div>

      <div id="splitTables">
        <table class="children sortable" id="id1807">
  
  
  <tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.tokenize.regexp.BlanklineTokenizer.html">BlanklineTokenizer</a></code></td>
    <td><span>Tokenize a string, treating any sequence of blank lines as a delimiter. Blank lines are defined as lines containing no characters, except for space or tab characters.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.tokenize.regexp.RegexpTokenizer.html">RegexpTokenizer</a></code></td>
    <td><span>A tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.tokenize.regexp.WhitespaceTokenizer.html">WhitespaceTokenizer</a></code></td>
    <td><span>Tokenize a string on whitespace (space, tab, newline). In general, users should use the string <tt class="rst-docutils literal">split()</tt> method instead.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.tokenize.regexp.WordPunctTokenizer.html">WordPunctTokenizer</a></code></td>
    <td><span>Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp <tt class="rst-docutils literal"><span class="pre">\w+|[^\w\s]+</span></tt>.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#regexp_tokenize">regexp_tokenize</a></code></td>
    <td><span>Return a tokenized copy of <em>text</em>.  See <a href="#id1"><span class="rst-problematic" id="rst-id2">:class:`.RegexpTokenizer`</span></a> for descriptions of the arguments.</span></td>
  </tr><tr class="variable">
    
    <td>Variable</td>
    <td><code><a href="#blankline_tokenize">blankline_tokenize</a></code></td>
    <td><span class="undocumented">Undocumented</span></td>
  </tr><tr class="variable">
    
    <td>Variable</td>
    <td><code><a href="#wordpunct_tokenize">wordpunct_tokenize</a></code></td>
    <td><span class="undocumented">Undocumented</span></td>
  </tr>
</table>
        

          
      </div>

      <div id="childList">

        <div class="basefunction">
  
  
  <a name="nltk.tokenize.regexp.regexp_tokenize">
    
  </a>
  <a name="regexp_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">regexp_tokenize</span>(text, pattern, gaps=False, discard_empty=True, flags=(re.UNICODE | re.MULTILINE | re.DOTALL)):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/regexp.py#L204">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div>Return a tokenized copy of <em>text</em>.  See <a href="#id1"><span class="rst-problematic" id="rst-id2">:class:`.RegexpTokenizer`</span></a>
for descriptions of the arguments.</div>
  </div>
</div><div class="basevariable">
  
  
  <a name="nltk.tokenize.regexp.blankline_tokenize">
    
  </a>
  <a name="blankline_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-defname">blankline_tokenize</span> =
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/regexp.py#L219">
      
      (source)
    </a>
  </div>
  <div class="functionBody">
    
    
    <div><p class="undocumented">Undocumented</p></div>
  </div>
</div><div class="basevariable">
  
  
  <a name="nltk.tokenize.regexp.wordpunct_tokenize">
    
  </a>
  <a name="wordpunct_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-defname">wordpunct_tokenize</span> =
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/regexp.py#L220">
      
      (source)
    </a>
  </div>
  <div class="functionBody">
    
    
    <div><p class="undocumented">Undocumented</p></div>
  </div>
</div>

      </div>
    </div>

    <footer class="navbar navbar-default">
  
  <div class="container">
    <a href="index.html">API Documentation</a> for <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a>,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    21.2.2 at 2021-06-22 02:56:13.
  </div>
</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>