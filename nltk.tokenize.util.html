<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  

  <head>
    
    <title>nltk.tokenize.util</title>
    <meta name="generator" content="pydoctor 21.2.2"> 
        
    </meta>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=0.75" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
    <link rel="stylesheet" type="text/css" href="extra.css" />
</head>

  <body>

    

    <nav class="navbar navbar-default">
  
  <div class="container">

    <div class="navbar-header">
      
      <div class="navlinks">
          <span class="navbar-brand">
            <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a> <a href="index.html">API Documentation</a>
          </span>

          <a href="moduleIndex.html">
            Modules
          </a>

          <a href="classIndex.html">
            Classes
          </a>

          <a href="nameIndex.html">
            Names
          </a>
      </div>

    </div>
  </div>
</nav>

    

    <div class="container">

      <div class="page-header">
        <h1 class="module"><code><code><a href="nltk.html">nltk</a></code><wbr></wbr>.<code><a href="nltk.tokenize.html">tokenize</a></code><wbr></wbr>.<code><a href="nltk.tokenize.util.html">util</a></code></code></h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <div class="categoryHeader">
        module documentation
      </div>

      <div class="extrasDocstring">
        <a href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/util.py" class="sourceLink">(source)</a>
        <p></p>
      </div>

      <div class="moduleDocstring">
        <div><p class="undocumented">Undocumented</p></div>
      </div>

      <div id="splitTables">
        <table class="children sortable" id="id1859">
  
  
  <tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.tokenize.util.CJKChars.html">CJKChars</a></code></td>
    <td><span>An object that enumerates the code points of the CJK characters as listed on <a class="rst-reference external" href="http://en.wikipedia.org/wiki/Basic_Multilingual_Plane#Basic_Multilingual_Plane" target="_top">http://en.wikipedia.org/wiki/Basic_Multilingual_Plane#Basic_Multilingual_Plane</a></span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#align_tokens">align_tokens</a></code></td>
    <td><span>This module attempt to find the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt> tuples, given the tokens and also the source string.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#is_cjk">is_cjk</a></code></td>
    <td><span>Python port of Moses' code to check for CJK character.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#regexp_span_tokenize">regexp_span_tokenize</a></code></td>
    <td><span>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt> tuples, by splitting the string at each successive match of <em>regexp</em>.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#spans_to_relative">spans_to_relative</a></code></td>
    <td><span>Return a sequence of relative spans, given a sequence of spans.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#string_span_tokenize">string_span_tokenize</a></code></td>
    <td><span>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt> tuples, by splitting the string at each occurrence of <em>sep</em>.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#xml_escape">xml_escape</a></code></td>
    <td><span>This function transforms the input text into an "escaped" version suitable for well-formed XML formatting.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#xml_unescape">xml_unescape</a></code></td>
    <td><span>This function transforms the "escaped" version suitable for well-formed XML formatting into humanly-readable string.</span></td>
  </tr>
</table>
        

          
      </div>

      <div id="childList">

        <div class="basefunction">
  
  
  <a name="nltk.tokenize.util.align_tokens">
    
  </a>
  <a name="align_tokens">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">align_tokens</span>(tokens, sentence):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/util.py#L257">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>This module attempt to find the offsets of the tokens in <em>s</em>, as a sequence
of <tt class="rst-docutils literal">(start, end)</tt> tuples, given the tokens and also the source string.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize <span class="py-keyword">import</span> TreebankWordTokenizer
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize.util <span class="py-keyword">import</span> align_tokens
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-builtin">str</span>(<span class="py-string">"The plane, bound for St Petersburg, crashed in Egypt's "</span>
<span class="py-more">... </span><span class="py-string">"Sinai desert just 23 minutes after take-off from Sharm el-Sheikh "</span>
<span class="py-more">... </span><span class="py-string">"on Saturday."</span>)
<span class="py-prompt">&gt;&gt;&gt; </span>tokens = TreebankWordTokenizer().tokenize(s)
<span class="py-prompt">&gt;&gt;&gt; </span>expected = [(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23),
<span class="py-more">... </span>(24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54),
<span class="py-more">... </span>(55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89),
<span class="py-more">... </span>(90, 98), (99, 103), (104, 109), (110, 119), (120, 122),
<span class="py-more">... </span>(123, 131), (131, 132)]
<span class="py-prompt">&gt;&gt;&gt; </span>output = <span class="py-builtin">list</span>(align_tokens(tokens, s))
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">len</span>(tokens) == <span class="py-builtin">len</span>(expected) == <span class="py-builtin">len</span>(output)  <span class="py-comment"># Check that length of tokens and tuples are the same.</span>
<span class="py-output">True</span>
<span class="py-prompt">&gt;&gt;&gt; </span>expected == <span class="py-builtin">list</span>(align_tokens(tokens, s))  <span class="py-comment"># Check that the output is as expected.</span>
<span class="py-output">True</span>
<span class="py-prompt">&gt;&gt;&gt; </span>tokens == [s[start:end] <span class="py-keyword">for</span> start, end <span class="py-keyword">in</span> output]  <span class="py-comment"># Check that the slices of the string corresponds to the tokens.</span>
<span class="py-output">True</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">tokens:</span>list(str)</td><td class="fieldArgDesc">The list of strings that are the result of tokenization</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">sentence:</span>str</td><td class="fieldArgDesc">The original string</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">list(tuple(int,int))</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.is_cjk">
    
  </a>
  <a name="is_cjk">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">is_cjk</span>(character):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/util.py#L162">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Python port of Moses' code to check for CJK character.</p>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>CJKChars().ranges
<span class="py-output">[(4352, 4607), (11904, 42191), (43072, 43135), (44032, 55215), (63744, 64255), (65072, 65103), (65381, 65500), (131072, 196607)]</span>
<span class="py-prompt">&gt;&gt;&gt; </span>is_cjk(u<span class="py-string">'㏾'</span>)
<span class="py-output">True</span>
<span class="py-prompt">&gt;&gt;&gt; </span>is_cjk(u<span class="py-string">'﹟'</span>)
<span class="py-output">False</span>
</pre><table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">character:</span>char</td><td class="fieldArgDesc">The character that needs to be checked.</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td colspan="2">bool</td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.regexp_span_tokenize">
    
  </a>
  <a name="regexp_span_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">regexp_span_tokenize</span>(s, regexp):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/util.py#L47">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt>
tuples, by splitting the string at each successive match of <em>regexp</em>.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize.util <span class="py-keyword">import</span> regexp_span_tokenize
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">'''Good muffins cost $3.88\nin New York.  Please buy me</span>
<span class="py-more">... </span><span class="py-string">two of them.\n\nThanks.'''</span>
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(regexp_span_tokenize(s, r<span class="py-string">'\s'</span>))
<span class="py-output">[(0, 4), (5, 12), (13, 17), (18, 23), (24, 26), (27, 30), (31, 36),</span>
<span class="py-output">(38, 44), (45, 48), (49, 51), (52, 55), (56, 58), (59, 64), (66, 73)]</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">s:</span>str</td><td class="fieldArgDesc">the string to be tokenized</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">regexp:</span>str</td><td class="fieldArgDesc">regular expression that matches token separators (must not be empty)</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">iter(tuple(int, int))</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.spans_to_relative">
    
  </a>
  <a name="spans_to_relative">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">spans_to_relative</span>(spans):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/util.py#L74">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Return a sequence of relative spans, given a sequence of spans.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize <span class="py-keyword">import</span> WhitespaceTokenizer
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize.util <span class="py-keyword">import</span> spans_to_relative
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">'''Good muffins cost $3.88\nin New York.  Please buy me</span>
<span class="py-more">... </span><span class="py-string">two of them.\n\nThanks.'''</span>
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(spans_to_relative(WhitespaceTokenizer().span_tokenize(s)))
<span class="py-output">[(0, 4), (1, 7), (1, 4), (1, 5), (1, 2), (1, 3), (1, 5), (2, 6),</span>
<span class="py-output">(1, 3), (1, 2), (1, 3), (1, 2), (1, 5), (2, 7)]</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">spans:</span>iter(tuple(int, int))</td><td class="fieldArgDesc">a sequence of (start, end) offsets of the tokens</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">iter(tuple(int, int))</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.string_span_tokenize">
    
  </a>
  <a name="string_span_tokenize">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">string_span_tokenize</span>(s, sep):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/util.py#L13">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="rst-docutils literal">(start, end)</tt>
tuples, by splitting the string at each occurrence of <em>sep</em>.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> nltk.tokenize.util <span class="py-keyword">import</span> string_span_tokenize
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">'''Good muffins cost $3.88\nin New York.  Please buy me</span>
<span class="py-more">... </span><span class="py-string">two of them.\n\nThanks.'''</span>
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-builtin">list</span>(string_span_tokenize(s, <span class="py-string">" "</span>))
<span class="py-output">[(0, 4), (5, 12), (13, 17), (18, 26), (27, 30), (31, 36), (37, 37),</span>
<span class="py-output">(38, 44), (45, 48), (49, 55), (56, 58), (59, 73)]</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">s:</span>str</td><td class="fieldArgDesc">the string to be tokenized</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">sep:</span>str</td><td class="fieldArgDesc">the token separator</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">iter(tuple(int, int))</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.xml_escape">
    
  </a>
  <a name="xml_escape">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">xml_escape</span>(text):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/util.py#L194">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>This function transforms the input text into an "escaped" version suitable
for well-formed XML formatting.</p>
<p>Note that the default xml.sax.saxutils.escape() function don't escape
some characters that Moses does so we have to manually add them to the
entities dictionary.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span>input_str = <span class="py-string">''')| &amp; &lt; &gt; ' " ] ['''</span>
<span class="py-prompt">&gt;&gt;&gt; </span>expected_output =  <span class="py-string">''')| &amp;amp; &amp;lt; &amp;gt; ' " ] ['''</span>
<span class="py-prompt">&gt;&gt;&gt; </span>escape(input_str) == expected_output
<span class="py-output">True</span>
<span class="py-prompt">&gt;&gt;&gt; </span>xml_escape(input_str)
<span class="py-output">')&amp;#124; &amp;amp; &amp;lt; &amp;gt; &amp;apos; &amp;quot; &amp;#93; &amp;#91;'</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">text:</span>str</td><td class="fieldArgDesc">The text that needs to be escaped.</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">str</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div><div class="basefunction">
  
  
  <a name="nltk.tokenize.util.xml_unescape">
    
  </a>
  <a name="xml_unescape">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">xml_unescape</span>(text):
    <a class="sourceLink" href="https://github.com/tristanlatr/nltk/tree/03a34b1932460e632ce9048adc0ccabbc7c0558c/nltk/tokenize/util.py#L226">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p>This function transforms the "escaped" version suitable
for well-formed XML formatting into humanly-readable string.</p>
<p>Note that the default xml.sax.saxutils.unescape() function don't unescape
some characters that Moses does so we have to manually add them to the
entities dictionary.</p>
<blockquote>
<pre class="py-doctest">
<span class="py-prompt">&gt;&gt;&gt; </span><span class="py-keyword">from</span> xml.sax.saxutils <span class="py-keyword">import</span> unescape
<span class="py-prompt">&gt;&gt;&gt; </span>s = <span class="py-string">')&amp;#124; &amp;amp; &amp;lt; &amp;gt; &amp;apos; &amp;quot; &amp;#93; &amp;#91;'</span>
<span class="py-prompt">&gt;&gt;&gt; </span>expected = <span class="py-string">''')| &amp; &lt; &gt; ' " ] ['''</span>
<span class="py-prompt">&gt;&gt;&gt; </span>xml_unescape(s) == expected
<span class="py-output">True</span>
</pre></blockquote>
<table class="fieldTable"><tr class="fieldStart"><td class="fieldName" colspan="2">Parameters</td></tr><tr><td class="fieldArgContainer"><span class="fieldArg">text:</span>str</td><td class="fieldArgDesc">The text that needs to be unescaped.</td></tr><tr class="fieldStart"><td class="fieldName" colspan="2">Returns</td></tr><tr><td class="fieldArgContainer">str</td><td class="fieldArgDesc"><span class="undocumented">Undocumented</span></td></tr></table></div>
  </div>
</div>

      </div>
    </div>

    <footer class="navbar navbar-default">
  
  <div class="container">
    <a href="index.html">API Documentation</a> for <a href="https://github.com/tristanlatr/nltk" class="projecthome">Natural Language Toolkit</a>,
  generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a>
    21.2.2 at 2021-06-22 02:56:13.
  </div>
</footer>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>