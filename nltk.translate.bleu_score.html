<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
          "DTD/xhtml1-strict.dtd">
<html>
  <head>
    <title>nltk.translate.bleu_score : API documentation</title>

    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="apidocs.css" />
  </head>
  <body>

    <nav class="navbar navbar-default">
      <div class="container">
        <div class="navbar-header navbar-brand">
          <a href="https://github.com/tristanlatr/nltk">Natural Language Toolkit</a>
          <a href="index.html">API Documentation</a>
        </div>
      </div>
    </nav>

    <div class="container">

      <div class="page-header">
        <h1 class="module"><code><code><a href="nltk.html">nltk</a></code>.<code><a href="nltk.translate.html">translate</a></code>.<code><a href="nltk.translate.bleu_score.html">bleu_score</a></code></code></h1>
        <div id="showPrivate">
          <button class="btn btn-link" onclick="togglePrivate()">Toggle Private API</button>
        </div>
      </div>

      <div class="categoryHeader">
        module documentation
      </div>

      <div class="extrasDocstring">
        <a href="https://github.com/tristanlatr/nltk/tree/cab27f08bc0f78151e7dd16ec110d324f8c05a72/nltk/translate/bleu_score.py">(source)</a>
        <p></p>
      </div>

      <div class="moduleDocstring">
        <div><p class="pre">BLEU score implementation.</p></div>
      </div>

      <div id="splitTables">
        <table class="children sortable" id="id1864">
  
  <tr class="function">
    
    <td>Function</td>
    <td><code><a href="#sentence_bleu">sentence_bleu</a></code></td>
    <td><span class="undocumented">No summary</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#corpus_bleu">corpus_bleu</a></code></td>
    <td><span>Calculate a single corpus-level BLEU score (aka. system-level BLEU) for all the hypotheses and their respective references.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#modified_precision">modified_precision</a></code></td>
    <td><span>Calculate modified ngram precision.</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#closest_ref_length">closest_ref_length</a></code></td>
    <td><span>This function finds the reference that is the closest length to the hypothesis. The closest reference length is referred to as *r* variable from the brevity penalty formula in Papineni et. al. (2002)</span></td>
  </tr><tr class="function">
    
    <td>Function</td>
    <td><code><a href="#brevity_penalty">brevity_penalty</a></code></td>
    <td><span>Calculate brevity penalty.</span></td>
  </tr><tr class="class">
    
    <td>Class</td>
    <td><code><a href="nltk.translate.bleu_score.SmoothingFunction.html">SmoothingFunction</a></code></td>
    <td><span class="undocumented">No summary</span></td>
  </tr>
</table>
        

          
      </div>

      <div id="childList">

        <div class="basefunction">
  
  <a name="nltk.translate.bleu_score.sentence_bleu">
    
  </a>
  <a name="sentence_bleu">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">sentence_bleu</span>(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=None, auto_reweigh=False):
    <a class="functionSourceLink" href="https://github.com/tristanlatr/nltk/tree/cab27f08bc0f78151e7dd16ec110d324f8c05a72/nltk/translate/bleu_score.py#L21">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p class="pre">Calculate BLEU score (Bilingual Evaluation Understudy) from
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
"BLEU: a method for automatic evaluation of machine translation."
In Proceedings of ACL. http://www.aclweb.org/anthology/P02-1040.pdf

&gt;&gt;&gt; hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',
...               'ensures', 'that', 'the', 'military', 'always',
...               'obeys', 'the', 'commands', 'of', 'the', 'party']

&gt;&gt;&gt; hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',
...               'forever', 'hearing', 'the', 'activity', 'guidebook',
...               'that', 'party', 'direct']

&gt;&gt;&gt; reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',
...               'ensures', 'that', 'the', 'military', 'will', 'forever',
...               'heed', 'Party', 'commands']

&gt;&gt;&gt; reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',
...               'guarantees', 'the', 'military', 'forces', 'always',
...               'being', 'under', 'the', 'command', 'of', 'the',
...               'Party']

&gt;&gt;&gt; reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',
...               'army', 'always', 'to', 'heed', 'the', 'directions',
...               'of', 'the', 'party']

&gt;&gt;&gt; sentence_bleu([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS
0.5045...

If there is no ngrams overlap for any order of n-grams, BLEU returns the
value 0. This is because the precision for the order of n-grams without
overlap is 0, and the geometric mean in the final BLEU score computation
multiplies the 0 with the precision of other n-grams. This results in 0
(independently of the precision of the othe n-gram orders). The following
example has zero 3-gram and 4-gram overlaps:

&gt;&gt;&gt; round(sentence_bleu([reference1, reference2, reference3], hypothesis2),4) # doctest: +ELLIPSIS
0.0

To avoid this harsh behaviour when no ngram overlaps are found a smoothing
function can be used.

&gt;&gt;&gt; chencherry = SmoothingFunction()
&gt;&gt;&gt; sentence_bleu([reference1, reference2, reference3], hypothesis2,
...     smoothing_function=chencherry.method1) # doctest: +ELLIPSIS
0.0370...

The default BLEU calculates a score for up to 4-grams using uniform
weights (this is called BLEU-4). To evaluate your translations with
higher/lower order ngrams, use customized weights. E.g. when accounting
for up to 5-grams with uniform weights (this is called BLEU-5) use:

&gt;&gt;&gt; weights = (1./5., 1./5., 1./5., 1./5., 1./5.)
&gt;&gt;&gt; sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS
0.3920...

:param references: reference sentences
:type references: list(list(str))
:param hypothesis: a hypothesis sentence
:type hypothesis: list(str)
:param weights: weights for unigrams, bigrams, trigrams and so on
:type weights: list(float)
:param smoothing_function:
:type smoothing_function: SmoothingFunction
:param auto_reweigh: Option to re-normalize the weights uniformly.
:type auto_reweigh: bool
:return: The sentence-level BLEU score.
:rtype: float</p></div>
  </div>
</div><div class="basefunction">
  
  <a name="nltk.translate.bleu_score.corpus_bleu">
    
  </a>
  <a name="corpus_bleu">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">corpus_bleu</span>(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=None, auto_reweigh=False):
    <a class="functionSourceLink" href="https://github.com/tristanlatr/nltk/tree/cab27f08bc0f78151e7dd16ec110d324f8c05a72/nltk/translate/bleu_score.py#L103">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p class="pre">Calculate a single corpus-level BLEU score (aka. system-level BLEU) for all
the hypotheses and their respective references.

Instead of averaging the sentence level BLEU scores (i.e. macro-average
precision), the original BLEU metric (Papineni et al. 2002) accounts for
the micro-average precision (i.e. summing the numerators and denominators
for each hypothesis-reference(s) pairs before the division).

&gt;&gt;&gt; hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',
...         'ensures', 'that', 'the', 'military', 'always',
...         'obeys', 'the', 'commands', 'of', 'the', 'party']
&gt;&gt;&gt; ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',
...          'ensures', 'that', 'the', 'military', 'will', 'forever',
...          'heed', 'Party', 'commands']
&gt;&gt;&gt; ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',
...          'guarantees', 'the', 'military', 'forces', 'always',
...          'being', 'under', 'the', 'command', 'of', 'the', 'Party']
&gt;&gt;&gt; ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',
...          'army', 'always', 'to', 'heed', 'the', 'directions',
...          'of', 'the', 'party']

&gt;&gt;&gt; hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',
...         'interested', 'in', 'world', 'history']
&gt;&gt;&gt; ref2a = ['he', 'was', 'interested', 'in', 'world', 'history',
...          'because', 'he', 'read', 'the', 'book']

&gt;&gt;&gt; list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]
&gt;&gt;&gt; hypotheses = [hyp1, hyp2]
&gt;&gt;&gt; corpus_bleu(list_of_references, hypotheses) # doctest: +ELLIPSIS
0.5920...

The example below show that corpus_bleu() is different from averaging
sentence_bleu() for hypotheses

&gt;&gt;&gt; score1 = sentence_bleu([ref1a, ref1b, ref1c], hyp1)
&gt;&gt;&gt; score2 = sentence_bleu([ref2a], hyp2)
&gt;&gt;&gt; (score1 + score2) / 2 # doctest: +ELLIPSIS
0.6223...

:param list_of_references: a corpus of lists of reference sentences, w.r.t. hypotheses
:type list_of_references: list(list(list(str)))
:param hypotheses: a list of hypothesis sentences
:type hypotheses: list(list(str))
:param weights: weights for unigrams, bigrams, trigrams and so on
:type weights: list(float)
:param smoothing_function:
:type smoothing_function: SmoothingFunction
:param auto_reweigh: Option to re-normalize the weights uniformly.
:type auto_reweigh: bool
:return: The corpus-level BLEU score.
:rtype: float</p></div>
  </div>
</div><div class="basefunction">
  
  <a name="nltk.translate.bleu_score.modified_precision">
    
  </a>
  <a name="modified_precision">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">modified_precision</span>(references, hypothesis, n):
    <a class="functionSourceLink" href="https://github.com/tristanlatr/nltk/tree/cab27f08bc0f78151e7dd16ec110d324f8c05a72/nltk/translate/bleu_score.py#L224">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p class="pre">Calculate modified ngram precision.

The normal precision method may lead to some wrong translations with
high-precision, e.g., the translation, in which a word of reference
repeats several times, has very high precision.

This function only returns the Fraction object that contains the numerator
and denominator necessary to calculate the corpus-level precision.
To calculate the modified precision for a single pair of hypothesis and
references, cast the Fraction object into a float.

The famous "the the the ... " example shows that you can get BLEU precision
by duplicating high frequency words.

    &gt;&gt;&gt; reference1 = 'the cat is on the mat'.split()
    &gt;&gt;&gt; reference2 = 'there is a cat on the mat'.split()
    &gt;&gt;&gt; hypothesis1 = 'the the the the the the the'.split()
    &gt;&gt;&gt; references = [reference1, reference2]
    &gt;&gt;&gt; float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS
    0.2857...

In the modified n-gram precision, a reference word will be considered
exhausted after a matching hypothesis word is identified, e.g.

    &gt;&gt;&gt; reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',
    ...               'ensures', 'that', 'the', 'military', 'will',
    ...               'forever', 'heed', 'Party', 'commands']
    &gt;&gt;&gt; reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',
    ...               'guarantees', 'the', 'military', 'forces', 'always',
    ...               'being', 'under', 'the', 'command', 'of', 'the',
    ...               'Party']
    &gt;&gt;&gt; reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',
    ...               'army', 'always', 'to', 'heed', 'the', 'directions',
    ...               'of', 'the', 'party']
    &gt;&gt;&gt; hypothesis = 'of the'.split()
    &gt;&gt;&gt; references = [reference1, reference2, reference3]
    &gt;&gt;&gt; float(modified_precision(references, hypothesis, n=1))
    1.0
    &gt;&gt;&gt; float(modified_precision(references, hypothesis, n=2))
    1.0

An example of a normal machine translation hypothesis:

    &gt;&gt;&gt; hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',
    ...               'ensures', 'that', 'the', 'military', 'always',
    ...               'obeys', 'the', 'commands', 'of', 'the', 'party']

    &gt;&gt;&gt; hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',
    ...               'forever', 'hearing', 'the', 'activity', 'guidebook',
    ...               'that', 'party', 'direct']

    &gt;&gt;&gt; reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',
    ...               'ensures', 'that', 'the', 'military', 'will',
    ...               'forever', 'heed', 'Party', 'commands']

    &gt;&gt;&gt; reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',
    ...               'guarantees', 'the', 'military', 'forces', 'always',
    ...               'being', 'under', 'the', 'command', 'of', 'the',
    ...               'Party']

    &gt;&gt;&gt; reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',
    ...               'army', 'always', 'to', 'heed', 'the', 'directions',
    ...               'of', 'the', 'party']
    &gt;&gt;&gt; references = [reference1, reference2, reference3]
    &gt;&gt;&gt; float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS
    0.9444...
    &gt;&gt;&gt; float(modified_precision(references, hypothesis2, n=1)) # doctest: +ELLIPSIS
    0.5714...
    &gt;&gt;&gt; float(modified_precision(references, hypothesis1, n=2)) # doctest: +ELLIPSIS
    0.5882352941176471
    &gt;&gt;&gt; float(modified_precision(references, hypothesis2, n=2)) # doctest: +ELLIPSIS
    0.07692...


:param references: A list of reference translations.
:type references: list(list(str))
:param hypothesis: A hypothesis translation.
:type hypothesis: list(str)
:param n: The ngram order.
:type n: int
:return: BLEU's modified precision for the nth order ngram.
:rtype: Fraction</p></div>
  </div>
</div><div class="basefunction">
  
  <a name="nltk.translate.bleu_score.closest_ref_length">
    
  </a>
  <a name="closest_ref_length">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">closest_ref_length</span>(references, hyp_len):
    <a class="functionSourceLink" href="https://github.com/tristanlatr/nltk/tree/cab27f08bc0f78151e7dd16ec110d324f8c05a72/nltk/translate/bleu_score.py#L335">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p class="pre">This function finds the reference that is the closest length to the
hypothesis. The closest reference length is referred to as *r* variable
from the brevity penalty formula in Papineni et. al. (2002)

:param references: A list of reference translations.
:type references: list(list(str))
:param hyp_len: The length of the hypothesis.
:type hyp_len: int
:return: The length of the reference that's closest to the hypothesis.
:rtype: int</p></div>
  </div>
</div><div class="basefunction">
  
  <a name="nltk.translate.bleu_score.brevity_penalty">
    
  </a>
  <a name="brevity_penalty">
    
  </a>
  <div class="functionHeader">
    
    <span class="py-keyword">def</span> <span class="py-defname">brevity_penalty</span>(closest_ref_len, hyp_len):
    <a class="functionSourceLink" href="https://github.com/tristanlatr/nltk/tree/cab27f08bc0f78151e7dd16ec110d324f8c05a72/nltk/translate/bleu_score.py#L355">
      
      (source)
    </a>
  </div>
  <div class="docstring functionBody">
    
    
    <div><p class="pre">Calculate brevity penalty.

As the modified n-gram precision still has the problem from the short
length sentence, brevity penalty is used to modify the overall BLEU
score according to length.

An example from the paper. There are three references with length 12, 15
and 17. And a concise hypothesis of the length 12. The brevity penalty is 1.

    &gt;&gt;&gt; reference1 = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12
    &gt;&gt;&gt; reference2 = list('aaaaaaaaaaaaaaa')   # i.e. ['a'] * 15
    &gt;&gt;&gt; reference3 = list('aaaaaaaaaaaaaaaaa') # i.e. ['a'] * 17
    &gt;&gt;&gt; hypothesis = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12
    &gt;&gt;&gt; references = [reference1, reference2, reference3]
    &gt;&gt;&gt; hyp_len = len(hypothesis)
    &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)
    &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len)
    1.0

In case a hypothesis translation is shorter than the references, penalty is
applied.

    &gt;&gt;&gt; references = [['a'] * 28, ['a'] * 28]
    &gt;&gt;&gt; hypothesis = ['a'] * 12
    &gt;&gt;&gt; hyp_len = len(hypothesis)
    &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)
    &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len)
    0.2635971381157267

The length of the closest reference is used to compute the penalty. If the
length of a hypothesis is 12, and the reference lengths are 13 and 2, the
penalty is applied because the hypothesis length (12) is less then the
closest reference length (13).

    &gt;&gt;&gt; references = [['a'] * 13, ['a'] * 2]
    &gt;&gt;&gt; hypothesis = ['a'] * 12
    &gt;&gt;&gt; hyp_len = len(hypothesis)
    &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)
    &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS
    0.9200...

The brevity penalty doesn't depend on reference order. More importantly,
when two reference sentences are at the same distance, the shortest
reference sentence length is used.

    &gt;&gt;&gt; references = [['a'] * 13, ['a'] * 11]
    &gt;&gt;&gt; hypothesis = ['a'] * 12
    &gt;&gt;&gt; hyp_len = len(hypothesis)
    &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)
    &gt;&gt;&gt; bp1 = brevity_penalty(closest_ref_len, hyp_len)
    &gt;&gt;&gt; hyp_len = len(hypothesis)
    &gt;&gt;&gt; closest_ref_len =  closest_ref_length(reversed(references), hyp_len)
    &gt;&gt;&gt; bp2 = brevity_penalty(closest_ref_len, hyp_len)
    &gt;&gt;&gt; bp1 == bp2 == 1
    True

A test example from mteval-v13a.pl (starting from the line 705):

    &gt;&gt;&gt; references = [['a'] * 11, ['a'] * 8]
    &gt;&gt;&gt; hypothesis = ['a'] * 7
    &gt;&gt;&gt; hyp_len = len(hypothesis)
    &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)
    &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS
    0.8668...

    &gt;&gt;&gt; references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]
    &gt;&gt;&gt; hypothesis = ['a'] * 7
    &gt;&gt;&gt; hyp_len = len(hypothesis)
    &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)
    &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len)
    1.0

:param hyp_len: The length of the hypothesis for a single sentence OR the
sum of all the hypotheses' lengths for a corpus
:type hyp_len: int
:param closest_ref_len: The length of the closest reference for a single
hypothesis OR the sum of all the closest references for every hypotheses.
:type closest_ref_len: int
:return: BLEU's brevity penalty.
:rtype: float</p></div>
  </div>
</div>

      </div>
      <address>
        <a href="index.html">API Documentation</a> for <a href="https://github.com/tristanlatr/nltk">Natural Language Toolkit</a>, generated by <a href="https://github.com/twisted/pydoctor/">pydoctor</a> 21.2.2 at 2021-06-22 02:47:44.
      </address>

    </div>

    <script src="pydoctor.js" type="text/javascript"></script>

  </body>
</html>